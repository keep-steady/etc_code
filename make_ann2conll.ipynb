{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ann 파일을 conll 포맷으로 변환하기\n",
    "\n",
    "191015\n",
    "\n",
    "BMES 포맷(or BIO)\n",
    "\n",
    "[@AlphaBay Market#Company*] 와 같이 [@~#~*] 식으로 표현되있다\n",
    "\n",
    "입력 문장을 너으면, conll 포맷으로 변형시켜 list로 출력해준다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, re\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[@July 06, 2017 01:49:00 PM#Time_Published*]',\n",
       " 'A Dark Web Marketplace is Down and Users Suspect Foul Play',\n",
       " '[@AlphaBay Market#Company*], a popular darknet marketplace has been offline since Tuesday night sparking concerns from users that the site’s operators have [@stolen customer account funds#Attack_Objective*] and disappeared.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filename = 'test.ann'\n",
    "# sentences = []\n",
    "# with open(filename, 'r', encoding='UTF-8') as f:\n",
    "#     for line in f.readlines():\n",
    "#         line = line.strip()\n",
    "#         sentences.append(line)\n",
    "        \n",
    "# sentences[:3]\n",
    "\n",
    "sentences = ['[@July 06, 2017 01:49:00 PM#Time_Published*]',\n",
    "             'A Dark Web Marketplace is Down and Users Suspect Foul Play',\n",
    "             '[@AlphaBay Market#Company*], a popular darknet marketplace has been offline since Tuesday night sparking concerns from users that the site’s operators have [@stolen customer account funds#Attack_Objective*] and disappeared.']\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     8,
     149
    ]
   },
   "outputs": [],
   "source": [
    "## function\n",
    "# input  : sentence with tag\n",
    "# output : conll format\n",
    "#     'Evolution S-Company\\n',\n",
    "#     'marketplace O\\n',\n",
    "#     'in O\\n',\n",
    "#     '2015 O\\n',\n",
    "\n",
    "def make_ann2conll(tagedSentence, tagScheme=\"BMES\", seget_nltk_tokenize=True, entityRe=r'\\[[\\@\\$)].*?\\#.*?\\*\\](?!\\#)'):\n",
    "    ## input  : sentence\n",
    "    ## output : pairList\n",
    "#         ['AlphaBay B-Company\\n',\n",
    "#          'Market I-Company\\n',\n",
    "#          ', O\\n',\n",
    "#          'a O\\n',\n",
    "\n",
    "    newSent = tagedSentence.strip('\\n')  # \\n 떼고\n",
    "\n",
    "    # filterList : ['[@AlphaBay Market#Company*]', '[@stolen customer account funds#Attack_Objective*]']\n",
    "    filterList = re.findall(entityRe, newSent)  # '[@~*]' 찾고\n",
    "    newSentLength = len(newSent)  # 문장 길이 구하고\n",
    "    chunk_list = []\n",
    "    start_pos = 0\n",
    "    end_pos = 0\n",
    "\n",
    "    # annotation이 없는 문장이면\n",
    "    if len(filterList) == 0:\n",
    "        singleChunkList = []\n",
    "        singleChunkList.append(newSent)\n",
    "        singleChunkList.append(0)\n",
    "        singleChunkList.append(len(newSent))\n",
    "        singleChunkList.append(False)\n",
    "        # chunk_list : [문장, 0, 문장길이, False]\n",
    "        chunk_list.append(singleChunkList)\n",
    "        # 초기화\n",
    "        singleChunkList = []\n",
    "\n",
    "    ## annotation이 있는 문장이면\n",
    "    # filterList : ['[@AlphaBay Market#Company*]', '[@stolen customer account funds#Attack_Objective*]']\n",
    "    else:\n",
    "        for pattern in filterList:\n",
    "            # print pattern\n",
    "            singleChunkList = []\n",
    "            start_pos = end_pos + newSent[end_pos:].find(pattern)\n",
    "            end_pos = start_pos + len(pattern)\n",
    "            singleChunkList.append(pattern)\n",
    "            singleChunkList.append(start_pos)\n",
    "            singleChunkList.append(end_pos)\n",
    "            singleChunkList.append(True)\n",
    "            # chunk_list : [패턴, 패턴 시작위치, 패턴 끝위치, True]\n",
    "            # [['[@AlphaBay Market#Company*]', 0, 27, True],\n",
    "            # ['[@stolen customer account funds#Attack_Objective*]', 156, 206, True]]\n",
    "            chunk_list.append(singleChunkList)\n",
    "            singleChunkList = []\n",
    "\n",
    "    ## chunk_list format:\n",
    "    # full_list 형태, 순서대로 이어붙이면 된다\n",
    "    # [['[@AlphaBay Market#Company*]', 0, 27, True],\n",
    "    #  [', a popular darknet marketplace has been offline since Tuesday night sparking concerns from users that the site’s operators have ',\n",
    "    #   27,\n",
    "    #   156,\n",
    "    #   False],\n",
    "    #  ['[@stolen customer account funds#Attack_Objective*]', 156, 206, True],\n",
    "    #  [' and disappeared.', 206, 223, False]]\n",
    "\n",
    "    full_list = []\n",
    "    for idx in range(0, len(chunk_list)):\n",
    "        if idx == 0:\n",
    "            if chunk_list[idx][1] > 0:\n",
    "                full_list.append([newSent[0:chunk_list[idx][1]], 0, chunk_list[idx][1], False])\n",
    "                full_list.append(chunk_list[idx])\n",
    "            else:\n",
    "                full_list.append(chunk_list[idx])\n",
    "\n",
    "        # annotation이 있으면\n",
    "        else:\n",
    "            if chunk_list[idx][1] == chunk_list[idx-1][2]:\n",
    "                full_list.append(chunk_list[idx])\n",
    "            elif chunk_list[idx][1] < chunk_list[idx-1][2]:\n",
    "                print(\"ERROR: found pattern has overlap!\", chunk_list[idx][1], ' with ', chunk_list[idx-1][2])\n",
    "            else:\n",
    "                full_list.append([newSent[chunk_list[idx-1][2]:chunk_list[idx][1]], chunk_list[idx-1][2], chunk_list[idx][1], False])\n",
    "                full_list.append(chunk_list[idx])\n",
    "\n",
    "        if idx == len(chunk_list) - 1 :\n",
    "            if chunk_list[idx][2] > newSentLength:\n",
    "                print(\"ERROR: found pattern position larger than sentence length!\")\n",
    "            elif chunk_list[idx][2] < newSentLength:\n",
    "                full_list.append([newSent[chunk_list[idx][2]:newSentLength], chunk_list[idx][2], newSentLength, False])\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "    #######################################################################            \n",
    "    #######################################################################\n",
    "    pairList = []\n",
    "    for eachList in full_list:\n",
    "        # eachList : [@AlphaBay Market#Company*]', 0, 27, True]\n",
    "        # eachList[3] : True or False, 어노테이션이냐 아니냐\n",
    "\n",
    "        # 1. 어노테이션일 때\n",
    "        if eachList[3]:\n",
    "            # 쪼개고\n",
    "            # [@AlphaBay Market#Company*] -> ['AlphaBay Market', 'Company*']\n",
    "            contLabelList = eachList[0].strip('[@$]').rsplit('#', 1)\n",
    "\n",
    "            # 쪼갠 길이가 2가 아니면 에러 표시\n",
    "            if len(contLabelList) != 2:\n",
    "                print(\"Error: sentence format error!\")\n",
    "            # 'Company*' 에서 *를 뗴고 label 선언\n",
    "            # label : Company\n",
    "            label = contLabelList[1].strip('*')\n",
    "\n",
    "            # 두 단어 이상이 annotation 되있으면 쪼갠다, 'AlphaBay Market' -> ['AlphaBay', 'Market']\n",
    "            # nltk word tokenize를 이용하여 쪼갠다\n",
    "            if seget_nltk_tokenize:\n",
    "                contLabelList[0] = word_tokenize(contLabelList[0])\n",
    "            # 그냥 빈칸 단위로 쪼갠다\n",
    "            else:\n",
    "                contLabelList[0] = contLabelList[0].split()\n",
    "\n",
    "                \n",
    "            ## BIO, BMES 관련 태깅함수\n",
    "            outList = outputWithTagScheme(contLabelList[0], label, tagScheme)\n",
    "\n",
    "            # pairList : ['AlphaBay B-Company\\n', 'Market I-Company\\n']\n",
    "            for eachItem in outList:\n",
    "                pairList.append(eachItem)\n",
    "\n",
    "        # 2. 어노테이션일 아닐때\n",
    "        else:\n",
    "            # nltk word tokenize를 이용하여 쪼갠다\n",
    "            if seget_nltk_tokenize:\n",
    "                eachList[0] = word_tokenize(eachList[0])\n",
    "            # 그냥 빈칸 단위로 쪼갠다\n",
    "            else:\n",
    "                eachList[0] = eachList[0].split()        \n",
    "\n",
    "            for idx in range(0, len(eachList[0])):\n",
    "                basicContent = eachList[0][idx]\n",
    "\n",
    "                # 빈칸이면 마킹 안하고 패스\n",
    "                if basicContent == ' ':  continue\n",
    "                # 아무것도 아니니까 O 을 단다\n",
    "                pair = basicContent + ' ' + 'O\\n'\n",
    "                pairList.append(pair)\n",
    "                \n",
    "    return pairList\n",
    "\n",
    "\n",
    "## 태그 달기\n",
    "def outputWithTagScheme(input_list, label, tagScheme=\"BMES\"):\n",
    "    output_list = []\n",
    "    list_length = len(input_list)\n",
    "    if tagScheme==\"BMES\":\n",
    "        if list_length ==1:\n",
    "            pair = input_list[0]+ ' ' + 'S-' + label + '\\n'\n",
    "            output_list.append(pair)\n",
    "        else:\n",
    "            for idx in range(list_length):\n",
    "                if idx == 0:\n",
    "                    pair = input_list[idx]+ ' ' + 'B-' + label + '\\n'\n",
    "                elif idx == list_length -1:\n",
    "                    pair = input_list[idx]+ ' ' + 'E-' + label + '\\n'\n",
    "                else:\n",
    "                    pair = input_list[idx]+ ' ' + 'M-' + label + '\\n'\n",
    "                output_list.append(pair)\n",
    "    else:\n",
    "        for idx in range(list_length):\n",
    "            if idx == 0:\n",
    "                pair = input_list[idx]+ ' ' + 'B-' + label + '\\n'\n",
    "            else:\n",
    "                pair = input_list[idx]+ ' ' + 'I-' + label + '\\n'\n",
    "#             output_list.append(pair.encode('utf-8'))\n",
    "            output_list.append(pair)\n",
    "    return output_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[@AlphaBay Market#Company*], a popular darknet marketplace has been offline since Tuesday night sparking concerns from users that the site’s operators have [@stolen customer account funds#Attack_Objective*] and disappeared.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['AlphaBay B-Company\\n',\n",
       " 'Market E-Company\\n',\n",
       " ', O\\n',\n",
       " 'a O\\n',\n",
       " 'popular O\\n',\n",
       " 'darknet O\\n',\n",
       " 'marketplace O\\n',\n",
       " 'has O\\n',\n",
       " 'been O\\n',\n",
       " 'offline O\\n',\n",
       " 'since O\\n',\n",
       " 'Tuesday O\\n',\n",
       " 'night O\\n',\n",
       " 'sparking O\\n',\n",
       " 'concerns O\\n',\n",
       " 'from O\\n',\n",
       " 'users O\\n',\n",
       " 'that O\\n',\n",
       " 'the O\\n',\n",
       " 'site O\\n',\n",
       " '’ O\\n',\n",
       " 's O\\n',\n",
       " 'operators O\\n',\n",
       " 'have O\\n',\n",
       " 'stolen B-Attack_Objective\\n',\n",
       " 'customer M-Attack_Objective\\n',\n",
       " 'account M-Attack_Objective\\n',\n",
       " 'funds E-Attack_Objective\\n',\n",
       " 'and O\\n',\n",
       " 'disappeared O\\n',\n",
       " '. O\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = sentences[2]\n",
    "\n",
    "conll_format = make_ann2conll(line)\n",
    "\n",
    "print(line)\n",
    "conll_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
