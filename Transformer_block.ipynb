{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Transformer (GPT-2 architecture)\n",
    "    def __init__(self, embed_dim, hidden_dim, \n",
    "                 num_embed, num_pos, num_heads, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.token_embeddings = nn.Embedding(num_embed, embed_dim)\n",
    "        self.position_embedings = nn.Embedding(num_pos, embed_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.attentions, self.feed_forwards = nn.ModuleList(), nn.ModuleList()\n",
    "        self.ln_1, self.ln_2 = nn.ModuleList(), nn.ModuleList()  # layer norm\n",
    "        \n",
    "        for _ in range(num_layers):\n",
    "            # Multi Head Attention 모듈을 불러와서 추가\n",
    "            self.attentions.append(nn.ModuleList(\n",
    "                    nn.MultiheadAttention(embed_dim, num_heads, dropout=dropout)))\n",
    "            # Feed forward layer 추가\n",
    "            self.feed_forwards.append(nn.Sequential(\n",
    "                    nn.Linear(embed_dim, hidden_dim),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_dim, embed_dim)))\n",
    "            self.ln_1.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            self.ln_2.append(nn.LayerNorm(embed_dim, eps=1e-12))\n",
    "            \n",
    "    def forward(self, x):\n",
    "        # token, position embedding\n",
    "        positions = torch.arange(len(x), device=x.device).unsqueeze(-1)  # [len(x)] -> [len(x), 1]\n",
    "        h = self.token_embeddings(x)\n",
    "        h = h + self.position_embedings(position).expand_as(h)\n",
    "        h = self.dropout(h)\n",
    "        # attention mask를 씌우기 위해 (len(x), len(x)) 사이즈의, -Inf 행렬을 만든다\n",
    "#         tensor([[-inf, -inf, -inf],\n",
    "#                 [-inf, -inf, -inf],\n",
    "#                 [-inf, -inf, -inf]])\n",
    "        attn_mask = torch.full((len(x), len(x)), -float('Inf'), \n",
    "                               device=h.device, dtype = h.dtype)\n",
    "        # 길이만큼은 attention 이 영향받도록 0, padding 부분은 -Inf로 배정한다\n",
    "#         tensor([[0., -inf, -inf],\n",
    "#                 [0., 0., -inf],\n",
    "#                 [0., 0., 0.]])\n",
    "        attn_mask = torch.triu(attn_mask, diagonal=1)\n",
    "        \n",
    "        # for x, y in zip(a, b)\n",
    "#         a = [1,2,3,4,5]\n",
    "#         b = ['a','b','c','d','e']\n",
    "#         (x, y) = (1, 'a'), (2, 'b'), (3, 'c'), ...\n",
    "        for layer_norm_1, attention, layer_norm_2, feed_forward in zip(\n",
    "            self.ln_1, self.attentions, self.ln_2, self.feed_forwards):\n",
    "            \n",
    "            h = layer_norm_1(h)\n",
    "            x, _ = attention(h, h, h, attn_mask=attn_mask, need_weight=False)  # [target length, batch size, embed dim] \n",
    "            x = self.dropout(x)\n",
    "            h = x + h  # residual connection\n",
    "            \n",
    "            h = layer_norm_1(h)\n",
    "            x = feed_forward(h)\n",
    "            x = self.dropout(x)\n",
    "            h = x + h  # residual connection\n",
    "            \n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
