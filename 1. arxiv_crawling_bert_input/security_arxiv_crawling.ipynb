{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secbert를 위한 arxiv 데이터 준비 & 크롤링\n",
    "190718"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2015년부터 ,arxiv에서 security 관련된 모든 논문(만개)들의\n",
    "\n",
    "title, abstract를 모아서 bert의 입력형태인\n",
    "\n",
    "    <<doc>>\n",
    "    title\n",
    "    abstract\n",
    "\n",
    "형태로 저장한다\n",
    "\n",
    "- 크롤링 소스\n",
    "https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=security&terms-0-field=all&classification-computer_science=y&classification-eess=y&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2015&date-to_date=&date-date_type=submitted_date&abstracts=hide&size=50&order=-announced_date_first&start=9950\n",
    "\n",
    "- 방법 ref\n",
    "https://github.com/aisolab/simple_examples_for_text_mining/blob/master/Scrapping%20text%20mining%20papers%20in%20arXiv.py\n",
    "\n",
    "- Error시 참고\n",
    "    크롤링을 돌리다 보면, arxiv쪽에서 공격으로 간주하고 ip접근을 차단한다.\n",
    "    그러므로 vpn(nodevpn)을 사용하여 ip를 우회하여 크롤링하고,\n",
    "    끊기면 다시 ip를 바꿔 이어 시작하는 형태로 한다.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_output = 'security_arxiv_paper.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL 작성\n",
    "base_url = 'https://arxiv.org/search/advanced?advanced=&terms-0-operator=AND&terms-0-term=security&terms-0-field=all&classification-computer_science=y&classification-eess=y&classification-physics_archives=all&classification-include_cross_list=include&date-year=&date-filter_by=date_range&date-from_date=2015&date-to_date=&date-date_type=submitted_date&abstracts=hide&size=50&order=-announced_date_first'\n",
    "# 페이지 넘어가며 뒤에 붙는 주소\n",
    "url_list = ['']\n",
    "temp = list(range(50, 10000, 50))  # '', '&start=50', '&start=100', ...8950 -> 총 10,106개\n",
    "for i in temp: url_list.append('&start={}'.format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printable_text_true(text):\n",
    "    # printable 하면 False\n",
    "    # 이상한게 있으면 True\n",
    "    not_printable = 0\n",
    "    for char in text:\n",
    "        if char not in string.printable:\n",
    "            not_printable += 1\n",
    "    if not_printable == 0:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178 / 200 - 0 / 0, 2019-07-18 19:06:28\n",
      "Connection refused, wait 60 sec\n",
      "179 / 200 - 41 / 9, 2019-07-18 19:08:27\n",
      "180 / 200 - 88 / 12, 2019-07-18 19:09:25\n",
      "181 / 200 - 132 / 18, 2019-07-18 19:10:25\n",
      "182 / 200 - 180 / 20, 2019-07-18 19:11:26\n",
      "183 / 200 - 222 / 28, 2019-07-18 19:12:26\n",
      "184 / 200 - 268 / 32, 2019-07-18 19:13:29\n",
      "185 / 200 - 316 / 34, 2019-07-18 19:14:31\n",
      "186 / 200 - 358 / 42, 2019-07-18 19:15:35\n",
      "187 / 200 - 405 / 45, 2019-07-18 19:16:36\n",
      "188 / 200 - 450 / 50, 2019-07-18 19:17:38\n",
      "189 / 200 - 495 / 55, 2019-07-18 19:18:42\n",
      "190 / 200 - 539 / 61, 2019-07-18 19:19:43\n",
      "191 / 200 - 579 / 71, 2019-07-18 19:20:46\n",
      "192 / 200 - 617 / 83, 2019-07-18 19:21:48\n",
      "193 / 200 - 660 / 90, 2019-07-18 19:22:51\n",
      "194 / 200 - 707 / 93, 2019-07-18 19:23:52\n",
      "195 / 200 - 751 / 99, 2019-07-18 19:24:55\n",
      "196 / 200 - 795 / 105, 2019-07-18 19:26:05\n",
      "197 / 200 - 839 / 111, 2019-07-18 19:27:08\n",
      "198 / 200 - 884 / 116, 2019-07-18 19:28:14\n",
      "199 / 200 - 928 / 122, 2019-07-18 19:29:17\n",
      "1430 sec\n",
      "965\n"
     ]
    }
   ],
   "source": [
    "# 처음 시작할땐 text 을 write 모드로 쓰기 위해 'w', \n",
    "# ip차단되어 vpn으로 ip바꾸고 다시 접속할땐 기존 text에 덮어쓰기 위해 'a'\n",
    "start_index = 178\n",
    "save_text = 'w' if start_index == 0 else 'a'\n",
    "\n",
    "file = open(save_output, save_text, encoding='UTF8')\n",
    "refused_wait_time = 60*1\n",
    "\n",
    "start = time.clock()\n",
    "count_doc, count_pass = 0, 0\n",
    "for i in range(start_index, len(url_list)):\n",
    "    print('%d / %d - %d / %d, %.19s' %(i, len(url_list), count_doc, count_pass, datetime.now()))\n",
    "    \n",
    "    tmp_url = base_url + url_list[i]  # 페이지 넘어가는 url 만듬\n",
    "    \n",
    "    ## 막히는 경우 예외처리\n",
    "    tmp_response = ''\n",
    "    while tmp_response == '':\n",
    "        try:\n",
    "            tmp_response = requests.get(tmp_url)\n",
    "            wait_time = random.randrange(8,11)\n",
    "            time.sleep(wait_time)\n",
    "            break\n",
    "        except:\n",
    "            print(\"Connection refused, wait {} sec\".format(refused_wait_time))\n",
    "            time.sleep(refused_wait_time)\n",
    "            continue            \n",
    "    \n",
    "    tmp_bs = BeautifulSoup(tmp_response.text, 'html.parser')\n",
    "    tmp_contents = tmp_bs.select('li.arxiv-result')\n",
    "    tmp_list = [tmp_href.select_one('a').attrs.get('href') for tmp_href in tmp_contents]\n",
    "\n",
    "    # ['https://arxiv.org/abs/1906.11488',\n",
    "    #  'https://arxiv.org/abs/1906.11465', ...]\n",
    "\n",
    "    for j, tmp_paper in enumerate(tmp_list):\n",
    "        # tmp_paper : archive 주소!\n",
    "        tmp_response_paper = requests.get(tmp_paper)  # <Response [200]>\n",
    "        tmp_bs_paper = BeautifulSoup(tmp_response_paper.text, 'html.parser')\n",
    "\n",
    "        title = re.sub('\\\\s+',\n",
    "                         ' ',\n",
    "                         tmp_bs_paper.select_one('h1.title.mathjax').text.replace('Title:', ''))\n",
    "        abstract = tmp_bs_paper.select_one('blockquote.abstract.mathjax').text.replace('Abstract:  ', '').replace('\\n', ' ')\n",
    "\n",
    "        if '$' in abstract or '$' in title or printable_text_true(title) or printable_text_true(abstract):\n",
    "            count_pass += 1\n",
    "            pass\n",
    "        # 다 printable 하면\n",
    "        else:\n",
    "            # 영어, 숫자, () 빼고 다 삭제\n",
    "            title = re.sub('[^a-zA-Z1-9()%\\' -%&,.]', '', title)\n",
    "            abstract = re.sub('[^a-zA-Z1-9()%\\' -%&,.]', '', abstract)\n",
    "            \n",
    "            file.write(\"<<doc>>\\n\")\n",
    "            file.write(title+'\\n')\n",
    "            file.write(abstract+'\\n\\n')\n",
    "            count_doc += 1\n",
    "\n",
    "end = time.clock()\n",
    "\n",
    "\n",
    "print('%d sec'%(end - start))\n",
    "print(count_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
