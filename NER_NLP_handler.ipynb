{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER 데이타셋을 만들기 위한 handler\n",
    "190619\n",
    "pdf -> plaintext 가 주어지면\n",
    "\n",
    "1) 문장단위로 분절\n",
    "\n",
    "2) 문장단위 분절 정제\n",
    "                                                                              \n",
    "3) 문장을 단어단위로 분절\n",
    " \n",
    "수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF reader tika\n",
    "pdf extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설치 및 사용법\n",
    "\n",
    "공식 홈페이지\n",
    "    - https://github.com/chrismattmann/tika-python\n",
    "사용법\n",
    "    - https://cnpnote.tistory.com/entry/PYTHON-PDF-%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8%EB%A5%BC-%EC%B6%94%EC%B6%9C%ED%95%98%EB%8A%94-%EB%B0%A9%EB%B2%95%EC%9D%80-%EB%AC%B4%EC%97%87%EC%9E%85%EB%8B%88%EA%B9%8C\n",
    "    - https://nearman.tistory.com/entry/8-%ED%8C%8C%EC%9D%B4%EC%8D%AC%EC%9C%BC%EB%A1%9C-PDF%ED%8C%8C%EC%9D%BC-%EC%9D%BD%EC%96%B4%EC%98%A4%EA%B8%B0-PDF%ED%8C%8C%EC%9D%BC%EC%97%90%EC%84%9C-%ED%85%8D%EC%8A%A4%ED%8A%B8-%EB%82%B4%EC%9A%A9-%ED%8C%8C%EC%8B%B1\n",
    "    - https://medium.com/@justinboylantoomey/fast-text-extraction-with-python-and-tika-41ac34b0fe61"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tika 설치, terminal 창에서 수행\n",
    "!pip install tika"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tika import parser\n",
    "\n",
    "raw = parser.from_file('dataset/sample.pdf')\n",
    "print(raw['content'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf -> txt\n",
    "parsed = parser.from_file(PDFfileName)\n",
    "if parsed[\"content\"]:\n",
    "    text = parsed[\"content\"]\n",
    "    print(text)\n",
    "else:\n",
    "    print('None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to string\n",
    "text = str(text)\n",
    "# Ensure text is utf-8 formatted\n",
    "safe_text = text.encode('utf-8', errors='ignore')\n",
    "# Escape any \\ issues\n",
    "safe_text = str(safe_text).replace('\\\\', '\\\\\\\\').replace('\"', '\\\\\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NLTK -> Spacy로 대체!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) 문장 단위 분절\n",
    "plaintxt 를 문장단위로 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다. 여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다. 전 그렇게 생각하지 않아요. 저는 여기 한 가지 문제점이 있다고 생각합니다. 왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠. 도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.2.5\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/87/76e691bbf1759ad6af5831649aae6a8d2fa184a1bcc71018ca6300399e5f/nltk-3.2.5.tar.gz (1.2MB)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from nltk==3.2.5) (1.12.0)\n",
      "Building wheels for collected packages: nltk\n",
      "  Building wheel for nltk (setup.py): started\n",
      "  Building wheel for nltk (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\go\\AppData\\Local\\pip\\Cache\\wheels\\da\\8c\\38\\a8a36581975f8d03275c49960019f955e4d19fd14ae7e42d3d\n",
      "Successfully built nltk\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.4\n",
      "    Uninstalling nltk-3.4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\programdata\\\\anaconda3\\\\envs\\\\venv\\\\lib\\\\site-packages\\\\nltk\\\\app\\\\chartparser_app.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\go\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.2.5\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = '현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다. 여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다. 전 그렇게 생각하지 않아요. 저는 여기 한 가지 문제점이 있다고 생각합니다. 왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠. 도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?'\n",
    "# if data.strip() != \"\":\n",
    "data = re.sub(r'([a-z])\\.([A-Z])', r'\\1. \\2', data.strip())\n",
    "data_sentences = sent_tokenize(data.strip())\n",
    "# sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.',\n",
       " '여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.',\n",
       " '전 그렇게 생각하지 않아요.',\n",
       " '저는 여기 한 가지 문제점이 있다고 생각합니다.',\n",
       " '왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.',\n",
       " '도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) 문장단위 분절 정제\n",
    "문장 합치기 및 분절 예제\n",
    "\n",
    "여러 라인에 한 문장이 들어 있는 경우에 대한 파이썬 스크립트 예제."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = ['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.',\n",
    "        '여기 계신 여러분의 대다수는',\n",
    "        '정말 대단한 일이라고 생각하시겠죠 --',\n",
    "        '전 다릅니다. 전 그렇게 생각하지 않아요.',\n",
    "        '저는 여기 한 가지 문제점이 있다고 생각합니다.',\n",
    "        '왜냐하면 강연이 1,000개라는 것은,',\n",
    "        '공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.',\n",
    "        '도대체 무슨 수로',\n",
    "        '1,000개나 되는 아이디어를 널리 알릴 건가요?',\n",
    "        '1,000개의 TED 영상 전부를 보면서']"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## !! label\n",
    "현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.\n",
    "여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.\n",
    "전 그렇게 생각하지 않아요.\n",
    "저는 여기 한 가지 문제점이 있다고 생각합니다.\n",
    "왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.\n",
    "도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?\n",
    "1,000개의 TED 영상 전부를 보면서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalized_sentence_tokenized = []\n",
    "\n",
    "buf = []\n",
    "for line in raw_data:\n",
    "    if line.strip() != \"\":\n",
    "        buf += [line.strip()]  # 공백 제거한 문장들 buf에 쌓기\n",
    "        sentences = sent_tokenize(\" \".join(buf))  # 글자별로 한칸씩 더 띄어쓰기\n",
    "        \n",
    "        if len(sentences) > 1:\n",
    "            buf = sentences[1:]  # 바로 뒷문장 buf로 가져오기\n",
    "#             print(sentences[0] + '\\n')\n",
    "            Normalized_sentence_tokenized.append(sentences[0] + '\\n')\n",
    "# print(\" \".join(buf) + \"\\n\")\n",
    "Normalized_sentence_tokenized.append(\" \".join(buf) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['현재 TED 웹사이트에는 1,000개가 넘는 TED강연들이 있습니다.\\n',\n",
       " '여기 계신 여러분의 대다수는 정말 대단한 일이라고 생각하시겠죠 -- 전 다릅니다.\\n',\n",
       " '전 그렇게 생각하지 않아요.\\n',\n",
       " '저는 여기 한 가지 문제점이 있다고 생각합니다.\\n',\n",
       " '왜냐하면 강연이 1,000개라는 것은, 공유할 만한 아이디어들이 1,000개 이상이라는 뜻이 되기 때문이죠.\\n',\n",
       " '도대체 무슨 수로 1,000개나 되는 아이디어를 널리 알릴 건가요?\\n',\n",
       " '1,000개의 TED 영상 전부를 보면서\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Normalized_sentence_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3) 문장을 단어단위로 분절\n",
    "문장을 단어 단위로 분절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk==3.2.5\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\envs\\venv\\lib\\site-packages (from nltk==3.2.5) (1.12.0)\n",
      "Installing collected packages: nltk\n",
      "  Found existing installation: nltk 3.4\n",
      "    Uninstalling nltk-3.4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] 액세스가 거부되었습니다: 'c:\\\\programdata\\\\anaconda3\\\\envs\\\\venv\\\\lib\\\\site-packages\\\\nltk\\\\app\\\\chartparser_app.py'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\go\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\go\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\nonbreaking_prefixes.zip.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk.tokenize.moses'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1843460bb07e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileinput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmoses\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMosesTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMosesTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'nltk.tokenize.moses'"
     ]
    }
   ],
   "source": [
    "!pip install nltk==3.2.5\n",
    "import nltk\n",
    "nltk.download('perluniprops')\n",
    "nltk.download('nonbreaking_prefixes')\n",
    "\n",
    "import sys, fileinput\n",
    "from nltk.tokenize.moses import MosesTokenizer\n",
    "\n",
    "tokenizer = MosesTokenizer()\n",
    "\n",
    "data = ['i love myself too much.', 'i am so hungry', 'where are you going']\n",
    "\n",
    "for line in data:\n",
    "    if line.strip() != \"\":\n",
    "        tokens = tokenizer.tokenize(line.strip(), escape=False)\n",
    "        tokens[-1] + \"\\n\"  # line 마지막에 \\n 삽입\n",
    "        print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
